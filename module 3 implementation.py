# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yrs4Wy4A8NkimOdfuJ17b8s_zeQzd7N-
"""

import numpy as np
import torch
from sklearn.metrics.pairwise import cosine_similarity
from torch.utils.data import DataLoader

from google.colab import drive
drive.mount('/content/drive')

import numpy as np

base_path = "/content/drive/MyDrive/ActiveLearningProject/data/"

uncertainty_scores = np.load(base_path + "uncertainty_scores.npy")
clipped_indices = np.load(base_path + "clipped_indices.npy")

print("Loaded clipped pool:", len(clipped_indices))

import torch
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

dataset = load_dataset("ag_news")
train_dataset = dataset["train"]

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4
).to(device)

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# same unlabeled subset size as Module-2
num_samples = len(train_dataset)
indices = np.arange(num_samples)

np.random.seed(42)
np.random.shuffle(indices)

labeled_size = int(0.05 * num_samples)

unlabeled_indices = indices[labeled_size:]

# use SAME 20000 subset as Module 2
unlabeled_dataset = train_dataset.select(unlabeled_indices[:20000])
tokenized_unlabeled = unlabeled_dataset.map(tokenize_function, batched=True)
tokenized_unlabeled.set_format(type="torch", columns=["input_ids", "attention_mask"])

clipped_dataset = tokenized_unlabeled.select(clipped_indices)
print("Clipped dataset size:", len(clipped_dataset))

clipped_dataset = clipped_dataset.select(range(2000))
print("Reduced clipped dataset:", len(clipped_dataset))

from torch.utils.data import DataLoader
import numpy as np

def get_embeddings(model, dataset_subset):
    loader = DataLoader(dataset_subset, batch_size=32)
    model.eval()

    embeddings = []

    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model.distilbert(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            cls_embed = outputs.last_hidden_state[:, 0, :]
            embeddings.append(cls_embed.cpu().numpy())

    return np.vstack(embeddings)

embeddings = get_embeddings(model, clipped_dataset)

print("Embeddings shape:", embeddings.shape)

from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(embeddings)
density_scores = similarity_matrix.mean(axis=1)

print("Density scores computed:", len(density_scores))

dense_ranked = np.argsort(-density_scores)

final_k = 50
similarity_threshold = 0.9

selected = []

for idx in dense_ranked:
    if len(selected) == 0:
        selected.append(idx)
        continue

    sims = similarity_matrix[idx, selected]

    if np.max(sims) < similarity_threshold:
        selected.append(idx)

    if len(selected) >= final_k:
        break

final_selected_indices = clipped_indices[selected]

print("Final selected samples:", len(final_selected_indices))

np.save(base_path + "final_selected_indices.npy", final_selected_indices)

print("Module 3 completed and saved to Drive")