# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QW4M2nnp-LVFChjdIWTrxfRylZLtftQz
"""

!pip install transformers datasets torch scikit-learn

import torch
import numpy as np
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.nn.functional import softmax
from torch.utils.data import DataLoader

dataset = load_dataset("ag_news")
train_dataset = dataset["train"]
test_dataset = dataset["test"]

print(len(train_dataset))

num_samples = len(train_dataset)
indices = np.arange(num_samples)

np.random.seed(42)
np.random.shuffle(indices)

labeled_size = int(0.05 * num_samples)

labeled_indices = indices[:labeled_size]
unlabeled_indices = indices[labeled_size:]

print("Labeled samples:", len(labeled_indices))
print("Unlabeled samples:", len(unlabeled_indices))

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4
)

print("Model Loaded Successfully")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

labeled_dataset = train_dataset.select(labeled_indices)

tokenized_labeled = labeled_dataset.map(tokenize_function, batched=True)

tokenized_labeled.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "label"]
)

print("Tokenized labeled data:", len(tokenized_labeled))

from torch.utils.data import DataLoader

labeled_loader = DataLoader(tokenized_labeled, batch_size=32, shuffle=True)

print("Labeled DataLoader ready")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("Using device:", device)

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

model.train()

for epoch in range(1):  # 1 epoch
    total_loss = 0

    for batch in labeled_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch finished. Avg loss:", total_loss / len(labeled_loader))

unlabeled_dataset = train_dataset.select(unlabeled_indices[:20000])

tokenized_unlabeled = unlabeled_dataset.map(tokenize_function, batched=True)

tokenized_unlabeled.set_format(
    type="torch",
    columns=["input_ids", "attention_mask"]
)

unlabeled_loader = DataLoader(tokenized_unlabeled, batch_size=32)

print("Unlabeled loader ready:", len(tokenized_unlabeled))

from torch.nn.functional import softmax

model.eval()

uncertainty_scores = []

with torch.no_grad():
    for batch in unlabeled_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        logits = outputs.logits
        probs = softmax(logits, dim=1)

        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)

        uncertainty_scores.extend(entropy.cpu().numpy())

uncertainty_scores = np.array(uncertainty_scores)

print("Uncertainty computed:", len(uncertainty_scores))

ranked_indices = np.argsort(-uncertainty_scores)

print("Top 5 uncertainty scores:")
print(uncertainty_scores[ranked_indices[:5]])

import os
os.makedirs("ActiveLearningProject/data", exist_ok=True)

print("Project folder created")

import numpy as np

np.save("ActiveLearningProject/data/uncertainty_scores.npy", uncertainty_scores)
np.save("ActiveLearningProject/data/ranked_indices.npy", ranked_indices)

print("Module 2 outputs saved successfully")

from google.colab import drive
drive.mount('/content/drive')

import os

project_path = "/content/drive/MyDrive/ActiveLearningProject"
os.makedirs(project_path + "/data", exist_ok=True)
os.makedirs(project_path + "/models", exist_ok=True)
os.makedirs(project_path + "/results", exist_ok=True)

print("Drive project folders created")

np.save(project_path + "/data/uncertainty_scores.npy", uncertainty_scores)
np.save(project_path + "/data/ranked_indices.npy", ranked_indices)

print("Module 2 saved permanently to Drive")

top_k = 1000  # choose 1000 most uncertain samples

top_uncertain_indices = ranked_indices[:top_k]

print("Selected top uncertain samples:", len(top_uncertain_indices))

def get_embeddings(model, dataset_subset):
    loader = DataLoader(dataset_subset, batch_size=32)
    model.eval()
    embeddings = []

    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model.distilbert(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            cls_embedding = outputs.last_hidden_state[:, 0, :]
            embeddings.append(cls_embedding.cpu().numpy())

    return np.vstack(embeddings)

top_uncertain_dataset = tokenized_unlabeled.select(top_uncertain_indices)

embeddings = get_embeddings(model, top_uncertain_dataset)

print("Embedding shape:", embeddings.shape)

from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(embeddings)

print("Similarity matrix shape:", similarity_matrix.shape)

density_scores = similarity_matrix.mean(axis=1)

print("Density scores computed:", len(density_scores))

final_k = 200

dense_ranked = np.argsort(-density_scores)

final_selected_indices = top_uncertain_indices[dense_ranked[:final_k]]

print("Final selected samples:", len(final_selected_indices))

np.save(project_path + "/data/final_selected_indices.npy", final_selected_indices)

print("Module 3 outputs saved permanently")

print("Uncertainty count:", len(uncertainty_scores))
print("Top uncertain selected:", len(top_uncertain_indices))
print("Final dense selected:", len(final_selected_indices))

# Convert back to original dataset indices

original_indices_subset = unlabeled_indices[:20000]

final_selected_original_indices = original_indices_subset[final_selected_indices]

print("Original dataset indices ready:", len(final_selected_original_indices))

np.save(project_path + "/data/final_selected_original_indices.npy",
        final_selected_original_indices)

print("Saved original indices for Module 1")

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ActiveLearningProject

!git init

!ls -a

!git remote add origin https://github.com/manashri834/ActiveLearningProject.git

!git remote -v

!git add .

!git commit -m "Initial project commit"

!git config --global user.email "manashri771@example.com"
!git config --global user.name "manashri834"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ActiveLearningProject

!git config --global user.email "manashri771@gmail.com"
!git config --global user.name "manashri834"

!git init
!git add .
!git commit -m "Initial commit"
!git branch -M main
!git remote add origin https://github.com/USERNAME/REPO.git
!git push -u origin main

!git remote set-url origin https://manashri834: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@github.com/manashri834/ActiveLearningProject.git
!git push -u origin main