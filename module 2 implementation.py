# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4n8lq-IVRFhOWixj7RnnWDZbqpQJDv7
"""

import torch
import numpy as np
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.utils.data import DataLoader
from torch.nn.functional import softmax
from torch.optim import AdamW
import os

from google.colab import drive
drive.mount('/content/drive')

dataset = load_dataset("ag_news")
train_dataset = dataset["train"]

num_samples = len(train_dataset)
indices = np.arange(num_samples)

np.random.seed(42)
np.random.shuffle(indices)

labeled_size = int(0.05 * num_samples)

labeled_indices = indices[:labeled_size]
unlabeled_indices = indices[labeled_size:]

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

labeled_dataset = train_dataset.select(labeled_indices)
tokenized_labeled = labeled_dataset.map(tokenize_function, batched=True)
tokenized_labeled.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

labeled_loader = DataLoader(tokenized_labeled, batch_size=32, shuffle=True)

optimizer = AdamW(model.parameters(), lr=5e-5)

model.train()
for batch in labeled_loader:
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["label"].to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("Initial training complete")

unlabeled_subset = train_dataset.select(unlabeled_indices[:20000])
tokenized_unlabeled = unlabeled_subset.map(tokenize_function, batched=True)
tokenized_unlabeled.set_format(type="torch", columns=["input_ids", "attention_mask"])

unlabeled_loader = DataLoader(tokenized_unlabeled, batch_size=32)

model.eval()
uncertainty_scores = []

with torch.no_grad():
    for batch in unlabeled_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits
        probs = softmax(logits, dim=1)

        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
        uncertainty_scores.extend(entropy.cpu().numpy())

uncertainty_scores = np.array(uncertainty_scores)

print("Entropy computed:", len(uncertainty_scores))

mean = np.mean(uncertainty_scores)
std = np.std(uncertainty_scores)

lower_bound = mean - 0.5 * std
upper_bound = mean + 1.0 * std

clipped_indices = np.where(
    (uncertainty_scores >= lower_bound) &
    (uncertainty_scores <= upper_bound)
)[0]

print("Samples after clipping:", len(clipped_indices))

os.makedirs("ActiveLearningProject/data", exist_ok=True)

np.save("ActiveLearningProject/data/uncertainty_scores.npy", uncertainty_scores)
np.save("ActiveLearningProject/data/clipped_indices.npy", clipped_indices)

print("Module 2 completed and saved")

import os
import numpy as np

base_path = "/content/drive/MyDrive/ActiveLearningProject/data/"
os.makedirs(base_path, exist_ok=True)

np.save(base_path + "uncertainty_scores.npy", uncertainty_scores)
np.save(base_path + "clipped_indices.npy", clipped_indices)

print("Module 2 saved permanently to Google Drive")

import os
print(os.listdir("/content/drive/MyDrive/ActiveLearningProject/data/"))